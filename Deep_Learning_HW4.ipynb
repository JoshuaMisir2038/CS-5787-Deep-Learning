{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC0S7XNqburNPHoUBIX2Io"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Joshua Misir - jm2448**\n",
        "\n",
        "**CS 5787 - Deep Learning**\n",
        "\n",
        "**Homework 4**\n",
        "\n",
        "**Spring 2023**"
      ],
      "metadata": {
        "id": "nlv7tUnfswQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 - Concepts (150 points total)**"
      ],
      "metadata": {
        "id": "ORXs5DTctT-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a self-attention mechanism and how is it used in Transformers?"
      ],
      "metadata": {
        "id": "KKsqTeJ1t9tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A self-attention mechanism is a neural network method that computes the weighted sum of the values of each in a sequence with respect to all the other elements in the sentence. \n",
        "\n",
        "Transformers are a type of neural network architecture that use self-attention mechanisms to encode an input sequence and genereate a corresponding output sequence. "
      ],
      "metadata": {
        "id": "j3d1tZ77uAU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  What is the difference between multi-head attention and single-head attention in Transformers?"
      ],
      "metadata": {
        "id": "X4RNx_FmJ654"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention mechanisms are used to allow the model to selectively focus on different parts of the input sequence during training and inference. \n",
        "\n",
        "Single-head attention is a traditional attention mechanism that computes a single attention weight matrix for each element in the sequence. This means that the model can attend to different parts of the input sequence, but only one aspect of the context can be captured at a time.\n",
        "\n",
        "Multi-head attention allows the model to attend to multiple parts of the input sequence simultaneously. Multi-head attention works by projecting the input sequence into multiple different subspaces and then computing attention on each subspace separately. This means that different aspects of the context can be captured at the same time, leading to better performance on certain tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "-CvgG8Y-J-KT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Suppose that we design a deep architecture to represent a sequence by stacking self-attention layers with positional encoding. What could be issues?"
      ],
      "metadata": {
        "id": "JvmMJj01Nt-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vanishing gradients - Deep architectures can suffer from the vanishing gradients problem, where the gradients used to update the weights of the lower layers become very small, making it difficult for the model to learn meaningful representations.\n",
        "\n",
        "- Computational complexity - Self-attention layers with positional encoding can be computationally expensive, especially when working with very long sequences. This can lead to increased training times and may require specialized hardware or distributed training techniques to be used. This will also have downstream effects: potentially being financially costly and resulting in excessive carbon emissions, if the energy source used for training stems from the use of fossil fuels. "
      ],
      "metadata": {
        "id": "YAwKNG2BN00K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. How would you design a learnable positional encoding method?"
      ],
      "metadata": {
        "id": "g1ZAmQFlQXpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a learnable function to generate positional embeddings. This function should take as input the position of each token in the sequence and output a learned embedding vector that captures positional information.\n",
        "\n",
        "Incorporate the learnable positional embeddings into the self-attention model. This can be done by adding the learned positional embeddings to the input embeddings of each token before passing them through the self-attention layers.\n",
        "\n",
        "Train the model end-to-end to optimize both the task-specific objective and the learnable positional embeddings. During training, the model should learn to generate positional embeddings that are tailored to the specific task and dataset, allowing it to capture more nuanced patterns in the data.\n",
        "\n",
        "Regularize the learned positional embeddings to prevent overfitting. This can be done using techniques such as weight decay or dropout, which encourage the model to learn more generalizable positional embeddings."
      ],
      "metadata": {
        "id": "XwsXywceQa4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.  Is it a good idea to replace scaled dot-product attention with additive attention in the Transformer? Why?"
      ],
      "metadata": {
        "id": "i4aYG0dKSIlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, this would not be a good idea. Dot-product attention has many benefits that additive attention does not have such as: \n",
        "\n",
        "1) computational efficiency advantage since it can be vectorized and computed in parallel across all query-key pairs \n",
        "\n",
        "2) Dot-product attention captures more complex relationships between the query and key vectors than additive attention since it takes into account the dot product of these vectors. "
      ],
      "metadata": {
        "id": "J_FAZ9aSSNUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. For language modeling, should we use the Transformer encoder, decoder, or both? How would you design this method?"
      ],
      "metadata": {
        "id": "u96FDfxcT5Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For language modeling, it is common to use the Transformer encoder as the base architecture, as it is optimized for processing input sequences in parallel and learning representations of them. The decoder is typically used for sequence generation tasks, such as machine translation.\n",
        "\n",
        "The steps to implement the Transformer encoder as the base architecture would look like the following:\n",
        "\n",
        "- Preprocess the input text by tokenizing it and converting the tokens to numerical indices.\n",
        "\n",
        "- Pass the numerical indices through an embedding layer to obtain dense representations for each token (this takes into account word similariy and distance of two given words).\n",
        "\n",
        "- Add positional encodings to the embedded tokens to provide information about the position of each token in the sequence.\n",
        "\n",
        "- Pass the positional embeddings through multiple layers of the Transformer encoder, which will learn to model the dependencies between tokens and generate context-aware representations.\n",
        "\n",
        "- Add a linear layer on top of the final Transformer encoder layer to generate predictions for each token in the sequence.\n",
        "\n",
        "- Train the model using a cross-entropy loss function, with the goal of predicting the next token in the sequence given the previous tokens.\n",
        "\n",
        "- During inference, the model can be used to generate text by iteratively predicting the next token and feeding it back into the model as input."
      ],
      "metadata": {
        "id": "EfCGLIduT_HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What can be challenges to Transformers if input sequences are very long? Why?"
      ],
      "metadata": {
        "id": "-cCytyhqWa0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers can face several challenges when processing input sequences that are very long. These challenges include:\n",
        "\n",
        "Memory requirements for large input sequences can be a challenge. Transformers use self-attention mechanisms to process input sequences, which requires storing all pairwise attention scores between each token and every other token in the sequence. This can quickly become computationally expensive and memory-intensive as the sequence length grows.\n",
        "\n",
        "Time needed to compute input sequences grows with large input sequences. Since Transformers process input sequences in parallel, the number of operations required to compute the self-attention scores grows quadratically with the sequence length. This means that as the sequence length grows, the computation time required for each self-attention layer increases significantly.\n",
        "\n",
        "Decrease in attention patterns. Self-attention mechanisms can become less effective when dealing with very long sequences, as the attention patterns become more diffuse and spread out. This can lead to a dilution of attention and a loss of focus on important parts of the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "JJvAH7qNWdVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Say that you are asked to fine tune a language model to perform text classification by adding additional layers. Where will you add them? Why?"
      ],
      "metadata": {
        "id": "2E073LdFXboL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I were asked to fine-tune a language model for text classification by adding additional layers, I would typically add the additional layers on top of the final output layer of the language model. This is known as a \"classification head\" and is a common technique for adapting pre-trained language models to downstream tasks such as text classification.\n",
        "\n",
        "The classification head typically consists of one or more fully connected layers followed by a softmax activation function to generate a probability distribution over the target classes. The input to the classification head would be the final output of the pre-trained language model, which would contain rich semantic information about the input text.\n",
        "\n",
        "By adding the additional layers on top of the final output layer of the language model, we are essentially \"fine-tuning\" the pre-trained model to better suit the specific task of text classification. This allows us to leverage the pre-trained model's knowledge of language semantics while still allowing the model to adapt to the specific classification task."
      ],
      "metadata": {
        "id": "JrYu9CgEXe6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. All other things being equal, will a masked language model require more or fewer pretraining steps to converge than a left-to-right language model? Why?"
      ],
      "metadata": {
        "id": "qGELFMQ0YqF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All other things being equal, a masked language model (MLM) is likely to require more pretraining steps to converge than a left-to-right (LTR) language model. The reason for this is that MLMs are inherently more complex than LTR models, as they require the model to predict missing words in the input text rather than simply generating text from left to right.\n",
        "\n",
        "In an LTR model, each token is generated conditioned only on the tokens that came before it in the input sequence. This makes the task relatively straightforward and can be trained using simple maximum likelihood estimation. In contrast, MLMs require the model to make predictions about each token in the input sequence, including tokens that are masked out and replaced with a special \"mask\" token.\n",
        "\n",
        "The additional complexity of MLMs means that they require more training steps to converge than LTR models. This is because the model must learn not only to generate text from left to right, but also to predict missing tokens in the input sequence. Additionally, the masking process can introduce noise into the training data, which can make the training process more difficult and require more training steps to achieve convergence."
      ],
      "metadata": {
        "id": "33dMyWlCYwQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Machine translation has long been evaluated based on superficial matching between an output translation and a ground-truth translation. How would you design a measure for evaluating machine translation results by using natural language inference?"
      ],
      "metadata": {
        "id": "FDHo1r86Zh7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we would obtain a set of translation outputs from a machine translation system and a corresponding set of source sentences in the original language.\n",
        "We would then use a pre-trained cross-lingual embedding model, such as XLM-R or mBERT, to encode both the source sentence and the translated output into a shared semantic space.\n",
        "\n",
        "Then, we would measure the semantic similarity between the source sentence and the translated output in the shared semantic space. One possible method for doing this is to compute the cosine similarity between the two embeddings.\n",
        "\n",
        "Finally, we would use the similarity score to evaluate the quality of the machine translation system. A higher similarity score would indicate that the machine translation system is producing more accurate translations.\n"
      ],
      "metadata": {
        "id": "OuWAurnuZpp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. How can we leverage BERT in training language models?\n",
        "\n"
      ],
      "metadata": {
        "id": "SfMLMRvcaBLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use BERT's pre-trained weights to initialize the weights of our own model, and then continue training it on our specific task or dataset (transfer learning). For example, we can fine-tune a pre-trained BERT model for sentiment analysis by adding a classification layer on top of the BERT output and training it on a sentiment analysis dataset.\n",
        "\n",
        "Another way to leverage BERT in training language models is to train it on multiple tasks simultaneously (multi-task learning). This can help improve the model's performance by allowing it to learn from multiple sources of information. For example, we can train BERT on a combination of tasks such as named entity recognition, question answering, and language modeling, all at the same time."
      ],
      "metadata": {
        "id": "BCkwuYLwaNjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Suppose that we have a trained model based on multi-head attention and we want to prune the least important attention heads to increase the prediction speed. How can we design experiments to measure the importance of an attention head?"
      ],
      "metadata": {
        "id": "MTQLJHGTakUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can mask out each head in turn and measure the effect on the model's performance. If the performance drops significantly after a particular head is masked out, we can conclude that the head is important for the model's prediction. This approach is similar to the technique of ablating individual neurons in a neural network to measure their importance.\n",
        "\n",
        "We can train the model with fewer attention heads and compare its performance to the original model. If the performance drop is small, we can conclude that the pruned heads were less important. Alternatively, if the performance drop is significant, we can conclude that the pruned heads were important for the model's prediction.\n",
        "\n",
        "We can analyze the attention maps generated by each head to determine which heads are focusing on important regions of the input. If a particular head consistently attends to unimportant regions, we can conclude that it is less important for the model's prediction."
      ],
      "metadata": {
        "id": "ZkgvUjYOazUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Consider sequence to sequence problems (e.g., machine translation) where the input sequence is always available throughout the target sequence prediction. What could be limitations of modeling with decoder-only Transformers? Why?"
      ],
      "metadata": {
        "id": "tyydIMYla5gH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One limitation is that they cannot take advantage of the input sequence when generating each output token. In decoder-only Transformers, each output token is generated based only on the previous output tokens generated by the model, not on the input sequence. This means that the model may not be able to use relevant information from the input sequence to improve the generation of the output sequence.\n",
        "\n",
        "Another limitation is that decoder-only Transformers may suffer from the problem of exposure bias. In the training process, decoder-only Transformers generate the output sequence token by token, with the true output sequence provided as input to the model. However, at inference time, the true output sequence is not available, and the model must generate the output sequence one token at a time based on its previous generated tokens. This discrepancy between training and inference can lead to the model making errors that it did not encounter during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "iBIu2y_ebJM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. What is the difference between a pre-trained Transformer model and a fine-tuned Transformer model?"
      ],
      "metadata": {
        "id": "DULt5-Pxbvmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-trained Transformer model is trained on a large corpus of text data with the goal of learning general language representations. The pre-training process usually involves a self-supervised learning task, such as masked language modeling, where the model learns to predict masked tokens in a given text sequence. The pre-training process enables the model to capture rich language representations that can be leveraged for various downstream tasks.\n",
        "\n",
        "Whereas, a fine-tuned Transformer model is a pre-trained Transformer model that has been further trained on a specific downstream task with the goal of improving its performance on that task. In the fine-tuning process, the pre-trained model is adapted to a specific task by adjusting its parameters on a smaller task-specific dataset, while keeping the pre-trained parameters fixed. Fine-tuning enables the model to capture task-specific information and improve its performance on the target task."
      ],
      "metadata": {
        "id": "vokc3yN_b0vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. You are going to use a LLM for an NLP task. What are the different ways you can use it? if you fine-tune it, what are the options?"
      ],
      "metadata": {
        "id": "8NJRihvkcXW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction: You can use the pre-trained LLM as a feature extractor by passing the input text through the model and obtaining the output representations at one of the layers. You can then use these representations as features in a downstream task, such as text classification or clustering.\n",
        "\n",
        "Fine-tuning: You can fine-tune the pre-trained LLM on a task-specific dataset by adding a task-specific output layer on top of the pre-trained model and training the whole model end-to-end. Fine-tuning enables the model to capture task-specific information and improve its performance on the target task.\n",
        "\n"
      ],
      "metadata": {
        "id": "I6-QjGXycc72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources:"
      ],
      "metadata": {
        "id": "CicJHUxkcje8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
        "- https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
        "- https://magazine.sebastianraschka.com/p/finetuning-large-language-models\n",
        "- https://sebastianraschka.com/blog/2023/llm-reading-list.html\n",
        "- https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
        "- https://chat.openai.com"
      ],
      "metadata": {
        "id": "G6eYjDO_cn0j"
      }
    }
  ]
}